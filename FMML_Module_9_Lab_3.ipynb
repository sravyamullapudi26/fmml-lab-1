{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravyamullapudi26/fmml-lab-1/blob/main/FMML_Module_9_Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 9: Convolutional Neural Networks\n",
        "## **Lab 3**\n",
        "### Module coordinator: Kushagra Agarwal"
      ],
      "metadata": {
        "id": "kCpbL40ggQf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Understanding Convolutions"
      ],
      "metadata": {
        "id": "0hAW8ptqVeyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/max/464/0*e-SMFTzO8r7skkpc\" width=650px/>"
      ],
      "metadata": {
        "id": "q6wfvhccKxWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yZD5S7IQgHbU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing some pytorch packages\n",
        "import torch\n",
        "from torch.nn import Conv2d"
      ],
      "metadata": {
        "id": "BDE4WBHalreb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central to CNNs, a convolution operation is a linear element-wise multiplication operation between a small filter/kernel and same-sized patch from the image. We move this filter over the image like a sliding window from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation. These filters can do simplest task like checking if there is a vertical line in the image or complicated task like detecting a human eye in the image.\n",
        "\n",
        "Let's look at the convolution formula:\n",
        "\n",
        "Convolution between image\n",
        "$f(x, y)$ and kernel $k(x, y)$ is\n",
        "$$f(x,y) * k(x,y) = \\sum \\limits _{i=0} ^{W-1} \\sum \\limits _{j=0} ^{H-1} f(i, j) k(x − i, y − j)$$\n",
        "\n",
        "where $W$ and $H$ are the the width and height of the image.\n",
        "\n",
        "The code demonstrates the convolution operation of a 2D matrix (image) with various filters"
      ],
      "metadata": {
        "id": "hbpRXyTpVv7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Chaim-Baskin/publication/318849314/figure/fig1/AS:614287726870532@1523469015098/Image-convolution-with-an-input-image-of-size-7-7-and-a-filter-kernel-of-size-3-3.png\" alt=\"Convolution\" width=650px height=280px/>"
      ],
      "metadata": {
        "id": "amI6DTS0Ksvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0], [1,1,0], [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0], [0,0,0], [1,1,1]])\n",
        "\n",
        "# On plotting the images\n",
        "plt.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()\n",
        "plt.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "IalqupPPkDil",
        "outputId": "11ffc3e7-a1ef-4642-edf7-5c6f261382f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42INjCaketK",
        "outputId": "5d7a0db5-e134-4a77-d989-674f8005601e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  3\n",
            "Output from second image:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tba3ySYUk2df",
        "outputId": "28411a2f-3366-4864-dd99-b298ebed5774"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  0\n",
            "Output from second image:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-zero output suggests that there is a vertical edge present in the first image and not present in the second image. Similarly, horizontal edge is detected in second."
      ],
      "metadata": {
        "id": "BmYcPhDgk_in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function to use convolution layer from Pytorch and use our own kernel to detect edges in image"
      ],
      "metadata": {
        "id": "UNdrDtAKqyj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_conv(image, kernel, padding=0, stride=1):\n",
        "\n",
        "  #--------IMAGE PREPROCESSING-------\n",
        "  image = torch.from_numpy(image)\n",
        "  # Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "  # --------------KERNEL-------------\n",
        "  kernel = torch.from_numpy(kernel.astype(np.float32))\n",
        "\n",
        "  # Pytorch requires kernel of shape (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  kernel = kernel.view((1,1,kernel.shape[0], kernel.shape[1]))\n",
        "\n",
        "  # ---------CONVOLUTION LAYER from Pytorch--------\n",
        "  conv = Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, padding=padding, stride=stride)\n",
        "\n",
        "  # Set the kernel weights in the convolution layer\n",
        "  conv.weight = torch.nn.Parameter(kernel)\n",
        "\n",
        "  # ---------APPLY CONVOLUTION--------\n",
        "  output = conv(input.float())\n",
        "  output_img = output.data.numpy()  # Tensor to back in numpy\n",
        "  output_img = output_img.reshape((-1, output_img.shape[-1])) # Reshape to 2D image\n",
        "\n",
        "  return output_img"
      ],
      "metadata": {
        "id": "G5fRJziBk3YB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def apply_conv(image, filter, padding=0, stride=1):\n",
        "    \"\"\"\n",
        "    Apply convolution operation on the image using the provided filter.\n",
        "\n",
        "    Args:\n",
        "    - image: Input image (2D numpy array).\n",
        "    - filter: Convolution filter (2D numpy array).\n",
        "    - padding: Amount of padding around the image (default: 0).\n",
        "    - stride: Stride value (default: 1).\n",
        "\n",
        "    Returns:\n",
        "    - out: Convolved image (2D numpy array).\n",
        "    \"\"\"\n",
        "    # Get dimensions of the input image and the filter\n",
        "    image_height, image_width = image.shape\n",
        "    filter_height, filter_width = filter.shape\n",
        "\n",
        "    # Calculate the dimensions of the output image after convolution\n",
        "    out_height = (image_height + 2 * padding - filter_height) // stride + 1\n",
        "    out_width = (image_width + 2 * padding - filter_width) // stride + 1\n",
        "\n",
        "    # Initialize the output image\n",
        "    out = np.zeros((out_height, out_width))\n",
        "\n",
        "    # Apply convolution operation\n",
        "    for i in range(0, out_height):\n",
        "        for j in range(0, out_width):\n",
        "            # Apply the filter to the current region of the image\n",
        "            image_patch = image[i * stride:i * stride + filter_height, j * stride:j * stride + filter_width]\n",
        "            out[i, j] = np.sum(image_patch * filter)\n",
        "\n",
        "    return out\n",
        "\n",
        "# Load the image\n",
        "image_path = '/content/grid1 (1).jpg'\n",
        "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "if image is None:\n",
        "    print(f\"Failed to load image from path: {image_path}\")\n",
        "else:\n",
        "    # Define the filters\n",
        "    filter1 = np.array([[-1,-1,-1],\n",
        "                        [ 0, 0, 0],\n",
        "                        [ 1, 1, 1]])\n",
        "\n",
        "    filter2 = np.array([[1,0,-1],\n",
        "                        [1,0,-1],\n",
        "                        [1,0,-1]])\n",
        "\n",
        "    # Apply convolution with the first filter\n",
        "    out1 = apply_conv(image, filter1, padding=0, stride=1)\n",
        "\n",
        "    # Apply convolution with the second filter\n",
        "    out2 = apply_conv(image, filter2, padding=0, stride=1)\n",
        "\n",
        "    # Display the results (you can use cv2.imshow() or matplotlib to display images)\n",
        "    cv2.imshow('Filtered Image 1', out1)\n",
        "    cv2.imshow('Filtered Image 2', out2)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0377Yu7y7Bbn",
        "outputId": "772aa10b-529f-4dd7-d65f-ed877dff02f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to load image from path: /content/grid1 (1).jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling Layers\n",
        "\n",
        "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.\n",
        "\n",
        "1) Max Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png' height=150px/>\n",
        "\n",
        "2) Average Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png' height=150px/>"
      ],
      "metadata": {
        "id": "FpA0yEk1BgRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax layer/activation\n",
        "Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.\n",
        "\n",
        "Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
        "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg' height=170px />"
      ],
      "metadata": {
        "id": "eu3QIU7AEO_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning to train a CNN network"
      ],
      "metadata": {
        "id": "P6grxC0TKKSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qlO-uZUHnn_-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Images returned from torchvision dataset classes is in range [0,1]\n",
        "# We transform them to tensors and normalize them to range [-1,1] using 'Normalize' transform\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "NnezCUbwGqzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3131bf4c-df31-406a-cfe8-e47b261c4a28"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 96249609.84it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training data shape : ', trainset.data.shape, len(trainset.targets))\n",
        "print('Testing data shape : ', testset.data.shape, len(testset.targets))\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "metadata": {
        "id": "e2M57DhHGupn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f239723-e3d2-4a02-b791-8925775ee7bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape :  (50000, 32, 32, 3) 50000\n",
            "Testing data shape :  (10000, 32, 32, 3) 10000\n",
            "Total number of outputs :  10\n",
            "Output classes :  ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "metadata": {
        "id": "_haw697lHCZs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))"
      ],
      "metadata": {
        "id": "x1Wi6vW7IHcR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        # output layer 10 classes\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RgxbRadcHIms"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "02meBxVOHLNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9c8b48-4d85-4fb6-b564-b1166dc4e338"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lfKHypeYHNHO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
      ],
      "metadata": {
        "id": "MuDnJL28HPKP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer)"
      ],
      "metadata": {
        "id": "AgKhwMrtHRCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebe9976-f40c-4db3-b0ec-f7ad79ac6bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.0130, Accuracy: 38.5420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "ax = fig.add_subplot(1,2, 2)\n",
        "ax.plot(np.arange(1,len(train_acc)+1),train_acc)\n",
        "plt.xlabel('Training accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Accuracy vs Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tM2wHKGuHToB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model, testloader)"
      ],
      "metadata": {
        "id": "3sHK9hhmI-VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "1) List some reasons why we should prefer CNN over ANN for image classification?\n",
        "\n",
        "2) Try improving the CNN performance further by tuning the hyperparameters(epochs, optimizer, LR etc). Report the improved test accuracy.\n",
        "\n",
        "3) What happens if you reduce the number of convolution layers to only 1?\n",
        "\n",
        "4) Why didn't we use the Softmax activation in the last layer of CNN?\n"
      ],
      "metadata": {
        "id": "RBQeCEB6REnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs) are generally preferred over Artificial Neural Networks (ANNs) for image classification tasks due to several reasons:\n",
        "\n",
        "1. **Hierarchical Feature Learning**: CNNs are specifically designed to automatically learn hierarchical representations of features from raw pixel values. They use convolutional layers to extract local features such as edges, textures, and patterns, and then aggregate them in subsequent layers to capture higher-level representations, which is crucial for understanding images.\n",
        "\n",
        "2. **Parameter Sharing**: CNNs utilize parameter sharing in their convolutional layers, which helps in reducing the number of parameters compared to fully connected layers in ANNs. This parameter sharing property allows CNNs to learn translation-invariant features, making them more efficient for processing images.\n",
        "\n",
        "3. **Spatial Hierarchies**: CNNs are well-suited for capturing spatial hierarchies present in images. The convolutional layers learn spatial relationships between pixels or regions, enabling them to detect features regardless of their location within the image. This spatial hierarchy is important for tasks like object detection and localization.\n",
        "\n",
        "4. **Local Receptive Fields**: CNNs employ local receptive fields in their convolutional layers, which enables them to focus on small, local regions of the input image at a time. This allows CNNs to efficiently capture local patterns and structures while preserving spatial information, which is crucial for understanding images.\n",
        "\n",
        "5. **Parameter Efficiency**: CNNs are more parameter-efficient compared to ANNs when dealing with large images. By sharing parameters across different regions of the input image, CNNs require fewer parameters to achieve similar or better performance compared to ANNs, making them more scalable for processing high-resolution images.\n",
        "\n",
        "6. **Transfer Learning**: CNNs trained on large datasets like ImageNet can be effectively used as feature extractors for various image classification tasks through transfer learning. By fine-tuning pre-trained CNNs on specific datasets, one can achieve good performance even with limited labeled data, which is beneficial in scenarios where labeled data is scarce.\n",
        "\n",
        "Overall, the architecture and design principles of CNNs make them highly suitable for image classification tasks, outperforming traditional ANNs in terms of performance, efficiency, and scalability."
      ],
      "metadata": {
        "id": "zYcZOeet6GXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DD2HPbSM6INU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tVcUjWJ6V5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the performance of a Convolutional Neural Network (CNN) for image classification, we can tune various hyperparameters such as the number of epochs, optimizer, learning rate, batch size, etc. Here's an example of how we can perform hyperparameter tuning using Python's Keras library:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Define the CNN architecture\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model with the specified optimizer\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "epochs = [10, 20, 30]\n",
        "optimizers = ['adam', 'sgd']\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "\n",
        "# Hyperparameter tuning loop\n",
        "for epoch in epochs:\n",
        "    for optimizer in optimizers:\n",
        "        for lr in learning_rates:\n",
        "            # Create the model with current hyperparameters\n",
        "            model = create_model(optimizer)\n",
        "            \n",
        "            # Train the model\n",
        "            history = model.fit(X_train, y_train, epochs=epoch, validation_data=(X_test, y_test), batch_size=32, verbose=0)\n",
        "\n",
        "            # Evaluate the model\n",
        "            test_accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "            \n",
        "            # Print the results\n",
        "            print(f'Epochs: {epoch}, Optimizer: {optimizer}, Learning Rate: {lr}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "            # Update best accuracy and hyperparameters if current model performs better\n",
        "            if test_accuracy > best_accuracy:\n",
        "                best_accuracy = test_accuracy\n",
        "                best_hyperparameters = {'epochs': epoch, 'optimizer': optimizer, 'learning_rate': lr}\n",
        "\n",
        "print(f\"\\nBest Test Accuracy: {best_accuracy:.4f}\")\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "```\n",
        "\n",
        "In this code:\n",
        "\n",
        "- We define a CNN architecture using Keras Sequential API.\n",
        "- We specify hyperparameters to tune such as the number of epochs, optimizer, and learning rate.\n",
        "- We use nested loops to iterate through all combinations of hyperparameters.\n",
        "- For each combination, we create a CNN model, train it on the training data, and evaluate its performance on the test data.\n",
        "- We print the test accuracy for each combination of hyperparameters.\n",
        "- Finally, we print the best test accuracy achieved and the corresponding best hyperparameters.\n",
        "\n",
        "You can run this code to perform hyperparameter tuning and find the combination of hyperparameters that results in the highest test accuracy. Adjust the hyperparameter ranges and search space as needed to further optimize the performance."
      ],
      "metadata": {
        "id": "1oJbU1kL6WZY"
      }
    }
  ]
}